#include "onnx_loader.hpp"
#include <fstream>
#include <stdexcept>
#include <vector>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/io/zero_copy_stream_impl.h>
#include "onnx.pb.h" // This will be generated by protoc

std::unordered_map<std::string, Tensor> load_onnx_initializers(const std::string& onnx_path) {
    std::ifstream input(onnx_path, std::ios::binary);
    if (!input) {
        throw std::runtime_error("Failed to open ONNX file: " + onnx_path);
    }

    // Read the entire file into a buffer.
    // This is not ideal for very large models, but fine for the MVP.
    std::vector<char> buffer((std::istreambuf_iterator<char>(input)), std::istreambuf_iterator<char>());

    onnx::ModelProto model_proto;

    // Protobuf messages can be very large, so we need to increase the limit.
    google::protobuf::io::ArrayInputStream raw_input(buffer.data(), buffer.size());
    google::protobuf::io::CodedInputStream coded_input(&raw_input);
    coded_input.SetTotalBytesLimit(INT_MAX); // Allow large models

    if (!model_proto.ParseFromCodedStream(&coded_input)) {
        throw std::runtime_error("Failed to parse ONNX model from: " + onnx_path);
    }

    std::unordered_map<std::string, Tensor> initializers;
    const auto& graph = model_proto.graph();

    for (const auto& initializer : graph.initializer()) {
        if (initializer.has_name()) {
            // In this phase, we just create dummy tensors.
            // Later, we will parse the data type and shape.
            initializers[initializer.name()] = Tensor();
        }
    }

    return initializers;
}